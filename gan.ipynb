{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavj98/machine-learning/blob/master/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yz1KDNMGTvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6M3kndXjTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import  os\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQuc4MSAXuf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3KeJ8-WaTqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train Images:\", train_images.shape, \"Test Images:\", test_images.shape)\n",
        "label_text = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RxGJQNWZk0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  j = randint(0,60000)\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.title(label_text[train_labels[j]])\n",
        "  plt.imshow(train_images[j], cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asheEAyuolwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0\n",
        "print(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Mzwp23aCtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN:\n",
        "  def make_model(self):\n",
        "    self.gen = self.generator()\n",
        "    self.dis = self.discriminator()\n",
        "        \n",
        "  def run_generator(self, z, is_training):\n",
        "    x = self.gen['dense1'](z)\n",
        "    x = self.gen['dropout1'](x, is_training)\n",
        "    x = self.gen['bnorm1'](x, is_training)\n",
        "    x = tf.reshape(x, (-1,7,7,1))\n",
        "    x = self.gen['convt1'](x)\n",
        "    x = self.gen['dropout2'](x, is_training)\n",
        "    x = self.gen['bnorm2'](x, is_training)\n",
        "    output = self.gen['convt2'](x)\n",
        "#     x = self.gen['dropout3'](x, is_training)\n",
        "#     x = self.gen['bnorm3'](x, is_training)\n",
        "#     x = self.gen['convt3'](x)\n",
        "#     x = self.gen['dropout4'](x, is_training)\n",
        "#     x = self.gen['bnorm4'](x, is_training)\n",
        "#     output = self.gen['convt4'](x)\n",
        "    return output\n",
        "  \n",
        "  def run_discriminator(self, z):\n",
        "    x = self.dis['conv1'](z)\n",
        "    x = self.dis['dropout1'](x)\n",
        "    x = self.dis['conv2'](x)\n",
        "    x = self.dis['dropout2'](x)\n",
        "    x = self.dis['conv3'](x)\n",
        "    x = self.dis['dropout3'](x)\n",
        "    x = self.dis['flatten'](x)\n",
        "    logits = self.dis['dense1'](x)\n",
        "    output = self.dis['logits'](x)\n",
        "    return logits, output\n",
        "  \n",
        "  def generator(self):\n",
        "    layers = {}\n",
        "    keep_prob = 0.3\n",
        "    momentum = 0.99\n",
        "    nodes = 7 * 7\n",
        "    layers['dense1'] = tf.keras.layers.Dense(units=nodes, activation=tf.nn.leaky_relu, name = 'gen/dense1')\n",
        "    layers['dropout1'] = tf.keras.layers.Dropout(keep_prob, name = 'gen/dropout1')      \n",
        "    layers['bnorm1'] = tf.keras.layers.BatchNormalization(momentum = momentum, name = 'gen/bnorm1')  \n",
        "    layers['convt1'] = tf.keras.layers.Conv2DTranspose(kernel_size=5, filters=64, strides=2, padding='same', activation=tf.nn.leaky_relu, name = 'gen/convt1')\n",
        "    layers['dropout2'] = tf.keras.layers.Dropout(keep_prob, name = 'gen/dropout2')\n",
        "    layers['bnorm2'] = tf.keras.layers.BatchNormalization(momentum=momentum, name = 'gen/bnorm2')\n",
        "    layers['convt2'] = tf.layers.Conv2DTranspose(kernel_size=5, filters=1, strides=2, padding='same', activation=tf.nn.leaky_relu, name = 'gen/convt2')\n",
        "    layers['dropout3'] = tf.keras.layers.Dropout(keep_prob, name = 'gen/dropout3')\n",
        "    layers['bnorm3'] = tf.keras.layers.BatchNormalization(momentum=momentum, name = 'gen/bnorm3')\n",
        "    layers['convt3'] = tf.keras.layers.Conv2DTranspose(kernel_size=5, filters=64, strides=1, padding='same', activation=tf.nn.leaky_relu, name = 'gen/convt3')\n",
        "    layers['dropout4'] = tf.keras.layers.Dropout(keep_prob, name = 'gen/dropout4')\n",
        "    layers['bnorm4'] = tf.keras.layers.BatchNormalization(momentum=momentum, name = 'gen/bnorm4')\n",
        "    layers['convt4'] = tf.keras.layers.Conv2DTranspose(kernel_size=5, filters=1, strides=1, padding='same', activation=tf.nn.sigmoid, name = 'gen/convt4')\n",
        "\n",
        "    return layers\n",
        "\n",
        "  def discriminator(self):\n",
        "    layers = {}\n",
        "    keep_prob = 0.7\n",
        "    layers['conv1'] = tf.keras.layers.Conv2D(kernel_size=5, filters=64, strides=2, padding='same', activation=tf.nn.leaky_relu, input_shape = (28,28,1), name = 'dis/conv1')\n",
        "    layers['dropout1'] = tf.keras.layers.Dropout(keep_prob, name = 'dis/dropout1')\n",
        "    layers['conv2'] = tf.keras.layers.Conv2D(kernel_size=5, filters=64, strides=1, padding='same', activation=tf.nn.leaky_relu, name = 'dis/conv2')\n",
        "    layers['dropout2'] = tf.keras.layers.Dropout(keep_prob,name = 'dis/dropout2')\n",
        "    layers['conv3'] = tf.keras.layers.Conv2D(kernel_size=5, filters=64, strides=1, padding='same', activation=tf.nn.leaky_relu, name = 'dis/conv3')\n",
        "    layers['dropout3'] = tf.keras.layers.Dropout(keep_prob, name = 'dis/dropout3')\n",
        "    layers['flatten'] = tf.keras.layers.Flatten(name = 'dis/flatten')\n",
        "    layers['dense1']=tf.keras.layers.Dense(units=128,activation=tf.nn.leaky_relu, name = 'dis/dense1')\n",
        "    layers['logits']=tf.keras.layers.Dense(units=1, activation = tf.nn.sigmoid, name = 'dis/logits')\n",
        "\n",
        "    return layers\n",
        "  \n",
        "#   def train_step_generator(self):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrBUYKbzPKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_func(logits_in,labels_in):\n",
        "  return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))\n",
        "  \n",
        "  \n",
        "def generate_fakes(n):\n",
        "  x = np.random.rand(n,28*28)\n",
        "  x = x*2-1\n",
        "  return x\n",
        "\n",
        "\n",
        "def dis_accuracy(D_logits, thresh, pos):\n",
        "  D_logits = D_logits.reshape(-1)\n",
        "  if not pos:\n",
        "    return len(D_logits[np.where(D_logits < thresh)])/len(D_logits)\n",
        "  else:\n",
        "    return len(D_logits[np.where(D_logits > thresh)])/len(D_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWvyq0Pe1QW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.get_default_graph()\n",
        "tf.reset_default_graph()\n",
        "fashion_GAN = GAN()\n",
        "fashion_GAN.make_model()\n",
        "\n",
        "\n",
        "# D_logits_fake= fashion_GAN.run_discriminator(fake_images)\n",
        "# D_logits_real= fashion_GAN.run_discriminator(real_images)\n",
        "# D_real_loss=loss_func(D_logits_real, tf.ones_like(D_logits_real)*0.9) #Smoothing for generalization\n",
        "# D_fake_loss=loss_func(D_logits_fake, tf.zeros_like(D_logits_fake))\n",
        "# D_loss=D_real_loss+D_fake_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-Xkz3d5cCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "real_images=tf.placeholder(tf.float32,shape=[None,28, 28, 1])\n",
        "# fake_images=tf.placeholder(tf.float32,shape=[None,28, 28, 1])\n",
        "z=tf.placeholder(tf.float32,shape=[None,100])\n",
        "\n",
        "G = fashion_GAN.run_generator(z, True)\n",
        "D_logits_real, D_output_real = fashion_GAN.run_discriminator(real_images)\n",
        "D_logits_fake, D_output_fake = fashion_GAN.run_discriminator(G)\n",
        "D_real_loss = loss_func(D_logits_real, tf.ones_like(D_logits_real)*0.9) #Smoothing for generalization\n",
        "D_fake_loss = loss_func(D_logits_fake, tf.zeros_like(D_logits_fake)+0.1)\n",
        "D_loss = (D_real_loss + D_fake_loss)/2\n",
        "G_loss = loss_func(D_logits_fake,tf.ones_like(D_logits_fake))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_qlohHMbLEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "samples=[] #generator examples\n",
        "D_losses = []\n",
        "G_losses = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXiG6PXlTav9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_D=0.002\n",
        "lr_G=0.002\n",
        "batch_size = 128\n",
        "#Do this when multiple networks interact with each other\n",
        "tvars=tf.trainable_variables()  #returns all variables created(the two variable scopes) and makes trainable true\n",
        "d_vars=[var for var in tvars if 'dis' in var.name]\n",
        "g_vars=[var for var in tvars if 'gen' in var.name]\n",
        "\n",
        "D_trainer=tf.train.AdamOptimizer(lr_D, 0.5).minimize(D_loss,var_list=d_vars)\n",
        "G_trainer=tf.train.AdamOptimizer(lr_G, 0.5).minimize(G_loss,var_list=g_vars)\n",
        "epochs=1000\n",
        "init=tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "print(d_vars, g_vars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLGOWQC7Ng9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  l = False\n",
        "  sess.run(init)\n",
        "  restore = True\n",
        "  num_batches = train_images.shape[0]//batch_size\n",
        "  if restore:\n",
        "    if True or os.path.exists(\"/content/drive/My Drive/mnist_gan_checkpt.ckpt\"):\n",
        "      saver.restore(sess, \"/content/drive/My Drive/mnist_gan_checkpt.ckpt\")\n",
        "      print(\"Restored model\")\n",
        "\n",
        "  print(\"TRAININGGGGGGGGG\")\n",
        " \n",
        "    \n",
        "  for epoch in range(epochs):\n",
        "    for i in range(num_batches):\n",
        "      batch = train_images[batch_size*i:(batch_size)*(i+1)]\n",
        "      batch_images = batch*2-1\n",
        "      batch_images = np.reshape(batch_images, (-1,28,28,1))\n",
        "      batch_z = np.random.uniform(-1,1,size=(batch_size,100))\n",
        "      sess.run(G_trainer, feed_dict={z:batch_z})\n",
        "      for l in range(5):\n",
        "        batch_z = np.random.uniform(-1,1,size=(batch_size,100))\n",
        "        sess.run(D_trainer, feed_dict = {real_images:batch_images,z:batch_z})\n",
        "      \n",
        "    print(\"on epoch{}\".format(epoch))\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "      print(\"Saving\")\n",
        "      save_path = saver.save(sess, \"/content/drive/My Drive/mnist_gan_checkpt.ckpt\")\n",
        "      batch_z = np.random.uniform(-1,1,size=(10000,100))\n",
        "      sample_z=np.random.uniform(-1,1,size=(9,100))\n",
        "      gen_sample=sess.run(G,feed_dict={z:sample_z})\n",
        "      # print(sample_z)\n",
        "      for k in range(9):\n",
        "        plt.subplot(3,3,k+1)\n",
        "        plt.imshow((gen_sample[k].reshape(28,28)+1)/2, cmap = 'gray')\n",
        "      plt.show()\n",
        "      D_real, D_fake, lossD, lossG = sess.run([D_logits_real, D_logits_fake, D_loss, G_loss], feed_dict={real_images:np.reshape(test_images*2-1, (-1,28,28,1)), z:batch_z})\n",
        "      D_acc_real = dis_accuracy(np.array(D_real), 0.5, True) \n",
        "      D_acc_fake = dis_accuracy(np.array(D_fake), 0.5, False)\n",
        "      G_acc = dis_accuracy(np.array(D_fake), 0.5, True)\n",
        "\n",
        "      print(\"lossD: {}, lossG: {}, accG: {}, accD: {}\".format(lossD, lossG, G_acc, (D_acc_real + D_acc_fake)/2))\n",
        "      samples.append(gen_sample)\n",
        "      D_losses.append(lossD)\n",
        "      G_losses.append(lossG)\n",
        "\n",
        "   \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkLBrn5-EKFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  restore = True\n",
        "  if restore:\n",
        "    if os.path.exists(\"/content/drive/My Drive/checkpoint\"):\n",
        "      saver.restore(sess, \"/content/drive/My Drive/checkpt.ckpt\")\n",
        "      print(\"Restored model\")\n",
        "    sample_z = np.random.uniform(-1,1,(10,100))\n",
        "    gen_sample=sess.run(fashion_GAN.run_generator(z, False),feed_dict={z:sample_z})\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ1LveN6EmFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_sample[1][:,:].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyw-fP9fEZc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(11):\n",
        "  plt.subplot(2,6,i+1)\n",
        "  plt.imshow(((gen_sample[i, :, :])))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xraGm5WTTuFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(epochs):\n",
        "    if epoch%100 == 0:\n",
        "      print(\"Saving\")\n",
        "#       save_path = saver.save(sess, \"/content/drive/My Drive/checkpt.ckpt\")\n",
        "      num_batches=train_images.shape[0]//batch_size\n",
        "    if epoch%5 == 0:\n",
        "        logits_fake = sess.run([D_logits_fake], feed_dict = {fake_images : np.random.uniform(-1,1,size=(train_images.shape[0],28,28,1))})\n",
        "        logits_real = sess.run([D_logits_real], feed_dict = {real_images : train_images.reshape(-1,28,28,1)})\n",
        "        print(\"Epoch : {}, Loss_fake : {}, Loss_real: {}\".format(epoch, dis_accuracy(np.array(logits_fake), 0.5, False), dis_accuracy(np.array(logits_real), 0.5, True)))\n",
        "    for i in range(num_batches):\n",
        "      batch = train_images[batch_size*i:(batch_size)*(i+1)]\n",
        "      batch_images = batch.reshape((batch_size,28,28,1))\n",
        "      batch_images = batch_images*2-1\n",
        "      batch_fake = np.random.uniform(-1,1,size=(batch_size,28,28,1))\n",
        "      lossD, _=sess.run([D_loss, D_trainer],feed_dict={real_images:batch_images,fake_images:batch_fake})\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6IAcf7sOLWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(samples[68].reshape(28,28))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKJlO0B_ALic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = []\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(100):\n",
        "      sample_z=np.random.uniform(-1,1,size=(1,100))\n",
        "      gen_sample=sess.run(fashion_GAN.run_generator(z),feed_dict={z:sample_z})\n",
        "      examples.append(gen_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRVbmqedTdlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(11):\n",
        "  plt.subplot(2,6,i+1)\n",
        "  plt.imshow((samples[i].reshape(28,28)+1)/2, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCjblcz-lBTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivfdeP-sc4rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.trainable_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ed2wIZtdghe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  a = sess.run(G, feed_dict = {z : np.random.uniform(-1,1,size=(1,100))})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w005LbcTrD28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lossD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOJXBEJEjr-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}